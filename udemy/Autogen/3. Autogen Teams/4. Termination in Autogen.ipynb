{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60482b51",
   "metadata": {},
   "source": [
    "# Terminations in Microsoft Autogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b7ed5",
   "metadata": {},
   "source": [
    "## Why Termination Matters\n",
    "\n",
    "We have used team to do some work, say write a story or accomplish some task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f86c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "model_client = OpenAIChatCompletionClient(model='gpt-4o', api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c48082",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e26d8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "add_1_agent_first = AssistantAgent(\n",
    "    name = 'add_1_agent_first',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number, first number is 0. Give result as output\"\n",
    ")\n",
    "\n",
    "add_1_agent_second = AssistantAgent(\n",
    "    name = 'add_1_agent_second',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number. Give result as output.\"\n",
    ")\n",
    " \n",
    "add_1_agent_third = AssistantAgent(\n",
    "    name = 'add_1_agent_third',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Add 1 to the number. Give result as output.\"\n",
    ")\n",
    "\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "\n",
    "team = RoundRobinGroupChat(\n",
    "    [add_1_agent_first, add_1_agent_second, add_1_agent_third]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87507801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for add_1_agent_first_0b0d3891-30b0-4f58-b2db-da0cc2a0674b/0b0d3891-30b0-4f58-b2db-da0cc2a0674b\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 510, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 748, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 870, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 523, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_agentchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mui\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(team.run_stream())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:482\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shutdown_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    480\u001b[39m         \u001b[38;5;66;03m# Wait for the shutdown task to finish.\u001b[39;00m\n\u001b[32m    481\u001b[39m         \u001b[38;5;66;03m# This will propagate any exceptions raised.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m shutdown_task\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    484\u001b[39m     \u001b[38;5;66;03m# Clear the output message queue.\u001b[39;00m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_message_queue.empty():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py:426\u001b[39m, in \u001b[36mBaseGroupChat.run_stream.<locals>.stop_runtime\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._runtime, SingleThreadedAgentRuntime)\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    425\u001b[39m     \u001b[38;5;66;03m# This will propagate any exceptions raised.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._runtime.stop_when_idle()\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    428\u001b[39m     \u001b[38;5;66;03m# Stop the consumption of messages and end the stream.\u001b[39;00m\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# NOTE: we also need to put a GroupChatTermination event here because when the group chat\u001b[39;00m\n\u001b[32m    430\u001b[39m     \u001b[38;5;66;03m# has an exception, the group chat manager may not be able to put a GroupChatTermination event in the queue.\u001b[39;00m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_message_queue.put(\n\u001b[32m    432\u001b[39m         GroupChatTermination(\n\u001b[32m    433\u001b[39m             message=StopMessage(content=\u001b[33m\"\u001b[39m\u001b[33mException occurred.\u001b[39m\u001b[33m\"\u001b[39m, source=\u001b[38;5;28mself\u001b[39m._group_chat_manager_name)\n\u001b[32m    434\u001b[39m         )\n\u001b[32m    435\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:746\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime.stop_when_idle\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRuntime is not started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_context.stop_when_idle()\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_context = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:120\u001b[39m, in \u001b[36mRunContext.stop_when_idle\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m._stopped.set()\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m._runtime._message_queue.shutdown(immediate=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_task\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:109\u001b[39m, in \u001b[36mRunContext._run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopped.is_set():\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._runtime._process_next()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:581\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_next\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    579\u001b[39m         e = \u001b[38;5;28mself\u001b[39m._background_exception\n\u001b[32m    580\u001b[39m         \u001b[38;5;28mself\u001b[39m._background_exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mmatch\u001b[39;00m message_envelope:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:528\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_publish\u001b[39m\u001b[34m(self, message_envelope)\u001b[39m\n\u001b[32m    525\u001b[39m         future = _on_message(agent, message_context)\n\u001b[32m    526\u001b[39m         responses.append(future)\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*responses)\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._ignore_unhandled_handler_exceptions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:523\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_publish.<locals>._on_message\u001b[39m\u001b[34m(agent, message_context)\u001b[39m\n\u001b[32m    515\u001b[39m logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing publish message for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    516\u001b[39m event_logger.info(\n\u001b[32m    517\u001b[39m     MessageHandlerExceptionEvent(\n\u001b[32m    518\u001b[39m         payload=\u001b[38;5;28mself\u001b[39m._try_serialize(message_envelope.message),\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m     )\n\u001b[32m    522\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:510\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_publish.<locals>._on_message\u001b[39m\u001b[34m(agent, message_context)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m MessageHandlerContext.populate_context(agent.id):\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m agent.on_message(\n\u001b[32m    511\u001b[39m             message_envelope.message,\n\u001b[32m    512\u001b[39m             ctx=message_context,\n\u001b[32m    513\u001b[39m         )\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    515\u001b[39m         logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError processing publish message for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_base_agent.py:113\u001b[39m, in \u001b[36mBaseAgent.on_message\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: Any, ctx: MessageContext) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_message_impl(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py:67\u001b[39m, in \u001b[36mSequentialRoutedAgent.on_message_impl\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fifo_lock.acquire()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().on_message_impl(message, ctx)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Release the FIFO lock to allow the next message to be processed.\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m._fifo_lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_routed_agent.py:485\u001b[39m, in \u001b[36mRoutedAgent.on_message_impl\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m h.router(message, ctx):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m h(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_unhandled_message(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_routed_agent.py:268\u001b[39m, in \u001b[36mevent.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    266\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in target types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m return_value = \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, message, ctx)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py:69\u001b[39m, in \u001b[36mChatAgentContainer.handle_request\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Pass the messages in the buffer to the delegate agent.\u001b[39;00m\n\u001b[32m     68\u001b[39m response: Response | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agent.on_messages_stream(\u001b[38;5;28mself\u001b[39m._message_buffer, ctx.cancellation_token):\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(msg, Response):\n\u001b[32m     71\u001b[39m         \u001b[38;5;66;03m# Log the response.\u001b[39;00m\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.publish_message(\n\u001b[32m     73\u001b[39m             GroupChatMessage(message=msg.chat_message),\n\u001b[32m     74\u001b[39m             topic_id=DefaultTopicId(\u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mself\u001b[39m._output_topic_type),\n\u001b[32m     75\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:748\u001b[39m, in \u001b[36mAssistantAgent.on_messages_stream\u001b[39m\u001b[34m(self, messages, cancellation_token)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;66;03m# STEP 3: Run the first inference\u001b[39;00m\n\u001b[32m    747\u001b[39m model_result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_llm(\n\u001b[32m    749\u001b[39m     model_client=model_client,\n\u001b[32m    750\u001b[39m     model_client_stream=model_client_stream,\n\u001b[32m    751\u001b[39m     system_messages=system_messages,\n\u001b[32m    752\u001b[39m     model_context=model_context,\n\u001b[32m    753\u001b[39m     tools=tools,\n\u001b[32m    754\u001b[39m     handoff_tools=handoff_tools,\n\u001b[32m    755\u001b[39m     agent_name=agent_name,\n\u001b[32m    756\u001b[39m     cancellation_token=cancellation_token,\n\u001b[32m    757\u001b[39m ):\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[32m    759\u001b[39m         model_result = inference_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py:870\u001b[39m, in \u001b[36mAssistantAgent._call_llm\u001b[39m\u001b[34m(cls, model_client, model_client_stream, system_messages, model_context, tools, handoff_tools, agent_name, cancellation_token)\u001b[39m\n\u001b[32m    868\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[32m    869\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m     model_result = \u001b[38;5;28;01mawait\u001b[39;00m model_client.create(\n\u001b[32m    871\u001b[39m         llm_messages, tools=all_tools, cancellation_token=cancellation_token\n\u001b[32m    872\u001b[39m     )\n\u001b[32m    873\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:523\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    522\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_beta_client:\n\u001b[32m    525\u001b[39m     result = cast(ParsedChatCompletion[Any], result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:2000\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1957\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1958\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1959\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1997\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1998\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   1999\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2001\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2002\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2003\u001b[39m             {\n\u001b[32m   2004\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2005\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2006\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2007\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2008\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2009\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2010\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2011\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2012\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2013\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2014\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2015\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2016\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2017\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2018\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2019\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2020\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2021\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2022\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2023\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2024\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2025\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2026\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2027\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2028\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2029\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2030\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2031\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2035\u001b[39m             },\n\u001b[32m   2036\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m   2037\u001b[39m         ),\n\u001b[32m   2038\u001b[39m         options=make_request_options(\n\u001b[32m   2039\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2040\u001b[39m         ),\n\u001b[32m   2041\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2042\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2043\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1767\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1755\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1762\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1763\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1764\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1765\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1766\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1461\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1459\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1462\u001b[39m     cast_to=cast_to,\n\u001b[32m   1463\u001b[39m     options=options,\n\u001b[32m   1464\u001b[39m     stream=stream,\n\u001b[32m   1465\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1466\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1467\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1547\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m err.response.aclose()\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1548\u001b[39m         input_options,\n\u001b[32m   1549\u001b[39m         cast_to,\n\u001b[32m   1550\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1551\u001b[39m         response_headers=err.response.headers,\n\u001b[32m   1552\u001b[39m         stream=stream,\n\u001b[32m   1553\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1554\u001b[39m     )\n\u001b[32m   1556\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1590\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1592\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1595\u001b[39m     options=options,\n\u001b[32m   1596\u001b[39m     cast_to=cast_to,\n\u001b[32m   1597\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1598\u001b[39m     stream=stream,\n\u001b[32m   1599\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1600\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1547\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m err.response.aclose()\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1548\u001b[39m         input_options,\n\u001b[32m   1549\u001b[39m         cast_to,\n\u001b[32m   1550\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1551\u001b[39m         response_headers=err.response.headers,\n\u001b[32m   1552\u001b[39m         stream=stream,\n\u001b[32m   1553\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1554\u001b[39m     )\n\u001b[32m   1556\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1590\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1592\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1595\u001b[39m     options=options,\n\u001b[32m   1596\u001b[39m     cast_to=cast_to,\n\u001b[32m   1597\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1598\u001b[39m     stream=stream,\n\u001b[32m   1599\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1600\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py:1562\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1559\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1561\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1565\u001b[39m     cast_to=cast_to,\n\u001b[32m   1566\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1570\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(team.run_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ea7a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "\n",
    "max_termination = MaxMessageTermination(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = RoundRobinGroupChat(\n",
    "    [add_1_agent_first, add_1_agent_second, add_1_agent_third],\n",
    "    termination_condition =max_termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5b0344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- add_1_agent_first ----------\n",
      "1\n",
      "---------- add_1_agent_second ----------\n",
      "2\n",
      "---------- add_1_agent_third ----------\n",
      "3\n",
      "---------- add_1_agent_first ----------\n",
      "4\n",
      "---------- add_1_agent_second ----------\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='add_1_agent_first', models_usage=RequestUsage(prompt_tokens=24, completion_tokens=2), metadata={}, content='1', type='TextMessage'), TextMessage(source='add_1_agent_second', models_usage=RequestUsage(prompt_tokens=29, completion_tokens=2), metadata={}, content='2', type='TextMessage'), TextMessage(source='add_1_agent_third', models_usage=RequestUsage(prompt_tokens=39, completion_tokens=2), metadata={}, content='3', type='TextMessage'), TextMessage(source='add_1_agent_first', models_usage=RequestUsage(prompt_tokens=55, completion_tokens=2), metadata={}, content='4', type='TextMessage'), TextMessage(source='add_1_agent_second', models_usage=RequestUsage(prompt_tokens=60, completion_tokens=2), metadata={}, content='5', type='TextMessage')], stop_reason='Maximum number of messages 5 reached, current message count: 5')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(team.run_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c9fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e48f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent1 = AssistantAgent(\n",
    "    name = 'story_writer',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Give the story about a brave knight, keep it short no more than 40 words. if critic say 'THE END' anywhere. Only output 'THE END'\"\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    name = 'story_critic',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Continue the story and critic it with feedback. Keep it short and no more than 40 words. If it feels complete, just say 'THE END'. Only output 'THE END'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84bc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "\n",
    "text_mention_termination = TextMentionTermination('THE END')\n",
    "teamWithTextTermination = RoundRobinGroupChat(\n",
    "    [agent1, agent2],\n",
    "    termination_condition=text_mention_termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "636e5e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a story about a brave knight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- story_writer ----------\n",
      "Sir Cedric, the brave knight, faced the fire-breathing dragon threatening his kingdom. With unwavering courage, he charged, sword gleaming. Against overwhelming odds, he triumphed, saving the realm and earning eternal admiration for his valor and selflessness.\n",
      "---------- story_critic ----------\n",
      "THE END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a story about a brave knight.', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=53, completion_tokens=54), metadata={}, content='Sir Cedric, the brave knight, faced the fire-breathing dragon threatening his kingdom. With unwavering courage, he charged, sword gleaming. Against overwhelming odds, he triumphed, saving the realm and earning eternal admiration for his valor and selflessness.', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=117, completion_tokens=3), metadata={}, content='THE END', type='TextMessage')], stop_reason=\"Text 'THE END' mentioned\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(teamWithTextTermination.run_stream(task = 'Write a story about a brave knight.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34697a41",
   "metadata": {},
   "source": [
    "# Combining Termination Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d08e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_termination = MaxMessageTermination(5) | TextMentionTermination('THE END')\n",
    "\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "\n",
    "\n",
    "text_mention_termination = TextMentionTermination('THE END')\n",
    "teamWithTextTermination = RoundRobinGroupChat(\n",
    "    [agent1, agent2],\n",
    "    termination_condition=combined_termination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446d0019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a story about a brave knight.\n",
      "---------- story_writer ----------\n",
      "Sir Rowan, the fearless knight, embarked on a quest to rescue the captured princess. Battling treacherous terrain and cunning foes, his determination never faltered. Through relentless bravery, he succeeded, restoring hope and earning the kingdom's eternal gratitude.\n",
      "---------- story_critic ----------\n",
      "THE END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a story about a brave knight.', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=302, completion_tokens=51), metadata={}, content=\"Sir Rowan, the fearless knight, embarked on a quest to rescue the captured princess. Battling treacherous terrain and cunning foes, his determination never faltered. Through relentless bravery, he succeeded, restoring hope and earning the kingdom's eternal gratitude.\", type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=383, completion_tokens=3), metadata={}, content='THE END', type='TextMessage')], stop_reason=\"Text 'THE END' mentioned\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(teamWithTextTermination.run_stream(task = 'Write a story about a brave knight.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663ef9b",
   "metadata": {},
   "source": [
    "# External Termination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85129fb8",
   "metadata": {},
   "source": [
    "ExternalTermination: Enables programmatic control of termination from outside the run. This is useful for UI integration (e.g., “Stop” buttons in chat interfaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1e8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "model_client = OpenAIChatCompletionClient(model='gpt-4o', api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6035d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "agent1 = AssistantAgent(\n",
    "    name = 'story_writer',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Give the story about a brave knight, keep it short no more than 40 words. if critic say 'THE END' anywhere. Only output 'THE END'\"\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    name = 'story_critic',\n",
    "    model_client=model_client,\n",
    "    system_message=\"Continue the story and critic it with feedback. Keep it short and no more than 40 words. If it feels complete, just say 'THE END'. Only output 'THE END'\"\n",
    ")\n",
    "\n",
    "from autogen_agentchat.conditions import ExternalTermination\n",
    "external_termination = ExternalTermination()\n",
    "\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "team = RoundRobinGroupChat(\n",
    "    [agent1, agent2],\n",
    "    termination_condition= external_termination\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd3f5df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a story about a brave knight less than 40 words.\n",
      "---------- story_critic ----------\n",
      "Sir Alden, the brave knight, ventured into the enchanted forest. Facing countless perils, he retrieved the mythical gem, saving the kingdom from eternal darkness. His legacy shone brighter than the gem itself. THE END\n",
      "---------- story_writer ----------\n",
      "THE END\n",
      "---------- story_critic ----------\n",
      "THE END\n",
      "---------- story_writer ----------\n",
      "THE END\n",
      "---------- story_critic ----------\n",
      "THE END\n",
      "---------- story_writer ----------\n",
      "THE END\n",
      "---------- story_critic ----------\n",
      "THE END\n",
      "---------- story_writer ----------\n",
      "THE END\n",
      "---------- story_critic ----------\n",
      "THE END\n",
      "---------- story_writer ----------\n",
      "THE END\n",
      "---------- story_critic ----------\n",
      "THE END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a story about a brave knight less than 40 words.', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=271, completion_tokens=45), metadata={}, content='Sir Alden, the brave knight, ventured into the enchanted forest. Facing countless perils, he retrieved the mythical gem, saving the kingdom from eternal darkness. His legacy shone brighter than the gem itself. THE END', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=319, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=332, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=338, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=351, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=357, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=370, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=376, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=389, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_writer', models_usage=RequestUsage(prompt_tokens=395, completion_tokens=3), metadata={}, content='THE END', type='TextMessage'), TextMessage(source='story_critic', models_usage=RequestUsage(prompt_tokens=408, completion_tokens=3), metadata={}, content='THE END', type='TextMessage')], stop_reason='External termination requested')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "run = asyncio.create_task(Console(team.run_stream(task = 'Write a story about a brave knight less than 40 words.')))\n",
    "\n",
    "await asyncio.sleep(10)\n",
    "\n",
    "external_termination.set()\n",
    "await run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8384e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The team is not stopping immediately but rather current agent is completing its run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef19875",
   "metadata": {},
   "source": [
    "# Aborting A Team\n",
    "\n",
    "Different from stopping a team, aborting a team will immediately stop the team and raise a CancelledError exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42abefe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Give a short Story about a lion atmost 40 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for story_critic_6ef0ad3e-c13a-4219-9e7f-21a8d50305f6/6ef0ad3e-c13a-4219-9e7f-21a8d50305f6\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 510, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 748, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 870, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 523, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1547, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1594, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/admin/Agentic-AI/Autogen/autogen-env/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Was Cancelled\n"
     ]
    }
   ],
   "source": [
    "from autogen_core import CancellationToken\n",
    "\n",
    "cancellation_token = CancellationToken()\n",
    "\n",
    "run2 = asyncio.create_task(\n",
    "    Console(team.run_stream(task = 'Give a short Story about a lion atmost 40 words',cancellation_token=cancellation_token))\n",
    ")\n",
    "\n",
    "await asyncio.sleep(5)\n",
    "cancellation_token.cancel()\n",
    "\n",
    "try:\n",
    "    result = await run2\n",
    "except :\n",
    "    print(\"Task Was Cancelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aeae06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
